---
title: "Replication Report"
author: "Hannah Hardenbergh"
date: "4/10/2019"
output:
  pdf_document:
---

```{r setup, include=FALSE}

#This is replication code for 'What the Demolition of Public Housing Teaches Us About
#the Impact of Racial Threat on Political Behavior', published June 2014 by Ryan
#Enos.

knitr::opts_chunk$set(echo = FALSE,
                      message=FALSE,
                      error=FALSE,
                      warning = FALSE)

# note: I took a look at Gabe's code to help me figure out how to display the
# stargazer table (Table 1) in the knitted pdf. I noticed that he used the echo,
# message, and error commands in almost every one of his code chunks. is there
# an easier way to do that so you don't have to re-write it for each chunk you
# want? to select chunk by numbers/names and tell the .rmd, this is the list of
# commands I want for each chunk? I'm just going to have all my chunks with the
# same list of commands, as seen above.

# load libraries

library(ei) 
library(MatchIt)
library(weights)
library(simpleboot)
library(Zelig)
library(boot)
library(apsrtable)
library(tidyverse)
library(stargazer)
```

This is Replication code for 'What the Demolition of Public Housing Teaches Us About
the Impact of Racial Threat on Political Behavior', published June 2014 by Ryan
Enos. I replicate all figures, Table 1, and provide the data for figure 1 and table 1. I do not replicate the Appendix figures. 

```{r}

# Let's get started. This code is from Enos' file called,
# "Create_tables_and_figures_only". Enos begins by making some parameters for
# his graphics.

ylims = c(-.35,.1)
ylims.2 = c(-.45,.1)
xlims = c(.5,11)
dists = seq(from = 1000, to = 100, by = -100) ###DELETE THIS LATER
xs = seq(1:length(dists))
ys = seq(from = -.35, to = .1, by = .05)
ys.lab = c('-0.35','-0.30', '-0.25','-0.20','-0.15','-0.10','-0.05','0.00','0.05','0.10')
ys.2 = seq(from = -.45, to = .1, by = .05)
ys.lab.2 = c('-0.45','-0.40','-0.35','-0.30', '-0.25','-0.20','-0.15','-0.10','-0.05','0.00','0.05','0.10')

offsets = .15
text.offsets = .025
cex.axis = .9
cex.N = .7
top.text.adj = c(1.3,1.3) ##offsets on labels to reduce crowding
bottom.text.adj = c(-.15,-.85)
point.size = 2
line.offset = .0175
```

```{r }

#Figure 1: Treatment Effects

#load data. make sure that the file path is correct. also, swap read.csv for
#read_csv every time you see it.

wtreat = read_csv('houses.data/white.treat.effect.mean.boot.csv') 
wtreat.lower = read_csv('houses.data/white.treat.effect.conf.boot.lower.csv') 
wtreat.upper = read_csv('houses.data/white.treat.effect.conf.boot.upper.csv') 
Nwtreat = read_csv('houses.data/white.treat.N.csv')
btreat = read_csv('houses.data/black.treat.effect.mean.boot.csv') 
btreat.lower = read_csv('houses.data/black.treat.effect.conf.boot.lower.csv') 
btreat.upper = read_csv('houses.data/black.treat.effect.conf.boot.upper.csv') 
Nbtreat = read_csv('houses.data/black.treat.N.csv')

#Enos uses letters for marking graphs for all except one

use.letters = c('a','b','c','d','e','f','skip','g','h')

##turning into matrices helps below with segment function
  
  use.wtreat = as.matrix(wtreat[7,])
  use.wlower = as.matrix(wtreat.lower[7,])
  use.wupper = as.matrix(wtreat.upper[7,])
  use.Nwtreat = as.matrix(Nwtreat[7,])
  
  use.btreat = as.matrix(btreat[7,])
  use.blower = as.matrix(btreat.lower[7,])
  use.bupper = as.matrix(btreat.upper[7,])
  use.Nbtreat = as.matrix(Nbtreat[7,])
  
  
  par(las = 1)
  par(mar = c(5.1, 4.1, .5, .5))
  plot(xs, use.wtreat,
       ylim = ylims,
       xlim = xlims,
       type = 'n',
       ylab = 'Treatment Effect',
       xlab = 'Treated Group Distance from Projects',
       xaxt = 'n',
       yaxt = 'n.csv')
  abline(h = 0, lty = 2)
  
  #draw lines first because I want them to be covered by points. create spaces in
  #lines using the offset (this allows the N to be displayed with the text()
  #function). black lines are offset to the left, white lines to the right
  
  segments(x0= xs[1:2]+offsets, x1 = xs[1:2]+offsets,
           y0 = use.btreat[,1:2], 
           y1 =	use.blower[,1:2])
  
  ##only do it for low N blacks because otherwise lines look funny
  
  segments(x0= xs[1:2]+offsets, x1 = xs[1:2]+offsets,
           y0 = use.btreat[,1:2] + line.offset, 
           y1 =	use.bupper[,1:2])
  
  ##now the others
  
  segments(x0= xs[3:10]+offsets, 
           x1 = xs[3:10]+offsets,
           y0 = use.blower[,3:10],
           y1 =	use.bupper[,3:10])
  
  segments(x0= xs-offsets, 
           x1 = xs-offsets,
           y0 = use.wtreat - line.offset, 
           y1 =	use.wlower)
  
  segments(x0= xs-offsets,
           x1 = xs-offsets,
           y0 = use.wtreat, 
           y1 =	use.wupper)
  
  ##points and N descriptions
  
  points(xs-offsets, use.wtreat,
         cex = point.size,
         pch = 21, 
         bg = 'white',
         col = 'black')
  
  text(xs-offsets,use.wtreat,
       paste('(',use.Nwtreat,')',sep = ''),
       cex = cex.N,
       #adj = top.text.adj
       pos = 1
  )
  
  points(xs+offsets, use.btreat,
         pch = 16,
         cex = point.size)
  
  text(xs+offsets,use.btreat,
       paste('(',use.Nbtreat,')',sep = ''),
       cex = cex.N,
       #adj = bottom.text.adj
       pos = 3
  )
  
  axis(side = 1,
       at = xs,
       label = seq(100,1000,100),
       cex.axis = cex.axis
  )
  axis(side = 2,
       at = ys,
       label = ys.lab,
       cex.axis = cex.axis
  )	
  
```

```{r }
#Figures 2: Treatment Effects Using Matched White Voters Near Nondemolished
#Projects for Control Group

##Enos' for loops cycled through a bunch of dataframes, each of which is needed for a
##different graph. The following is only dataframes needed for Figures 2 and 3.
##Get rid of all his for loops! because we aren't making the Appendix cr*p, you
##can just read in the data in a normal line of code (we only need one of his
##graphs.) (I'm not a fan of for loops....... why can't you just run code for
##certain conditions without one??)

## make data point types into triangles (for arbitrary reasons, as far as I can
## tell. aesthetics? his graphs are a bit boring...)
		
pchs = 17 

# read in data for figure 2.

treat <- read_csv('houses.data/white.match.nondemolished.csv')
diffs <- read_csv('houses.data/white.match.nondemolished.diffs.csv')


##define axis for different graphs
  
use.ylims = ylims
use.ys.lab = ys.lab
use.ys = ys


# this code finds confidence intervals: go through each pair of
# dataframes
  
use.treat = treat$coefficient			
clower = use.treat-(1.96*treat$stdev)
cupper = use.treat+(1.96*treat$stdev)
use.N.treat = treat$N.treatment + treat$N.control
		
# make parameters for these graphics
	 
par(las = 1)
par(mar = c(5.1, 4.1, .5, .5))
plot(xs, use.treat,
ylim = use.ylims,
xlim = xlims,
type = 'n',
ylab = 'Treatment Effect',
xlab = 'Treated Group Distance from Projects',
xaxt = 'n',
yaxt = 'n')

# Include a line at y = 0

abline(h = 0, lty = 2)

# this limits the segment lines for each confidence interval, I believe.

segments(x0=xs,
         x1=xs,
         y0= use.treat + line.offset,
         y1=cupper)

segments(x0=xs,
         x1=xs,
         y0= use.treat,
         y1=clower)

# print out triangle points

points(xs, use.treat, 
pch = pchs, 
cex = point.size,
bg = 'white',
col = 'black')
text(xs,use.treat,
paste('(',use.N.treat,')',sep = ''),
cex = cex.N,
pos = 3)

# print axes names

axis(side = 1,
					at = xs,
					label = seq(100,1000,100),
					cex.axis = cex.axis
					)
axis(side = 2,
					at = use.ys,
					label = use.ys.lab,
					cex.axis = cex.axis
					)
```


```{r}
# and Figure 3: Treatment Effects Using Matched Black Control Group and
# Controlling for Homeownership. Much of this code is repeated from how to build
# Figure 2.

# load data for figure 3. these object names are the same exact ones for figure
# 2 and 3. this worries me, but when I renamed them, I was going to have to
# rename several other places where "treat" is used. for now, I will leave it,
# because it doesn't seem to cause any problems. but still...

treat <- read_csv('houses.data/white.match.black.property.csv')
diffs <- read_csv('houses.data/white.match.black.diffs.property.csv')

##define axis for different graphs
  
use.ylims = ylims
use.ys.lab = ys.lab
use.ys = ys


# this code finds confidence intervals: go through each pair of
# dataframes
  
use.treat = treat$coefficient			
clower = use.treat-(1.96*treat$stdev)
cupper = use.treat+(1.96*treat$stdev)
use.N.treat = treat$N.treatment + treat$N.control
		
# make parameters for these graphics
	 
par(las = 1)
par(mar = c(5.1, 4.1, .5, .5))
plot(xs, use.treat,
ylim = use.ylims,
xlim = xlims,
type = 'n',
ylab = 'Treatment Effect',
xlab = 'Treated Group Distance from Projects',
xaxt = 'n',
yaxt = 'n')

# Include a line at y = 0

abline(h = 0, lty = 2)

# this limits the segment lines for each confidence interval, I believe.

segments(x0=xs,
         x1=xs,
         y0= use.treat + line.offset,
         y1=cupper)

segments(x0=xs,
         x1=xs,
         y0= use.treat,
         y1=clower)

# print out triangle points

points(xs, use.treat, 
pch = pchs, 
cex = point.size,
bg = 'white',
col = 'black')
text(xs,use.treat,
paste('(',use.N.treat,')',sep = ''),
cex = cex.N,
pos = 3)

# print axes names

axis(side = 1,
					at = xs,
					label = seq(100,1000,100),
					cex.axis = cex.axis
					)
axis(side = 2,
					at = use.ys,
					label = use.ys.lab,
					cex.axis = cex.axis
					)
```

```{r }
#predicted effects graphs, Figure 4

# load more data

distdat =  read_csv('houses.data/predicted.results.distance.vary.context.csv')

# so much data

areadat = read_csv('houses.data/predicted.results.area.vary.context.csv')

#new ylims for these graphs

ylims.predict = c(.6,.75)

#Enos likes to put data in a list to "cycle through" it

datas = list(distdat,areadat)

##parameters to be used in graphs below

xs = list(seq(from = 10, to = 2000, by = 10), 
          seq(from = 45000, to = 1004000, by = 4800)/1000)

use.letters = c('a','b')

xlabs = c('Distance from Project',
          'Percent of Local Black Population in Demolished Project')

ylabs = c(expression(Pr(vote[2004])),'')

vlines = list(seq(from = 0, to = 2000, by = 200),
              seq(from = 0, to = 1000, by = 100))

axis.labs = list(as.character(seq(from = 0, to = 2000, by = 200)),
                 
	as.character(c('0','10%','20%','30%','40%','50%','60%','70%','80%','90%','100%')))

for(i in 1:2){
  
  #saving renames columns, so name back
  
	colnames(datas[[i]]) = c("mean","sd","50%","2.5%","97.5%") 

		par(las = 1)
		par(mar = c(5.1, 4.1, .5, .5))
		plot(xs[[i]],datas[[i]][,'mean'],
			type = 'l',
			xlab = xlabs[i],
			ylab = ylabs[i],
			ylim = ylims.predict,
			xaxt = 'n',
			cex.axis = cex.axis,
			lwd = 4
		)
		
	#put horizontal and vertical lines on plots
		
	abline(h = seq(from = min(ylims.predict), to = max(ylims.predict), by = .025),
	       lty = 2,
	       col = 'gray',
	       lwd = 1)
	abline(v = vlines[[i]], 
	       lty = 2,
	       col = 'gray',
	       lwd = 1)
	lines(xs[[i]],datas[[i]][,'2.5%'],
			lty = 3,
			lwd = 2.5)
	lines(xs[[i]],datas[[i]][,'97.5%'],
			lty = 3,
			lwd = 2.5)
	axis(side = 1, 
		at = vlines[[i]], 
		labels = axis.labs[[i]],
		cex.axis = cex.axis)
		
}
```

```{r }
#vote choice graphs, Figures 5 and 6

pres.elections = c('dole_pct_ei','bush2000_pct_ei','bush2004_pct_ei','mccain_pct_ei')
obama.elections = c('obama_sen_primary_pct_ei','keyes_pct_ei','obama_pres_primary_pct_ei')

# Load! More! Data!

dists = read_csv('houses.data/distance.vote.differences.csv')
demos = read_csv('houses.data/demolished.vote.differences.csv')

graphs = c('5a','5b','6')

for(i in graphs){

	if(i == '5a'){dat = dists}
	else{dat = demos}
		
	if(i %in% c('5a','5b')){
		xlims = c(.75,4.25)
		ylims = c(-.1,.2)	
	}
  
	else{
		xlims = c(.75,3.25)
		ylims = c(-.1,.25)
	}
  

	##recode Keyes to Obama general for presentation purposes
  
	dat[dat$election == 'keyes_pct_ei','x.mean'] = 1 - dat[dat$election == 'keyes_pct_ei','x.mean']
	
	dat[dat$election == 'keyes_pct_ei','y.mean'] = 1 - dat[dat$election == 'keyes_pct_ei','y.mean']
	
	dat[dat$election == 'keyes_pct_ei','diff'] =dat[dat$election == 'keyes_pct_ei','y.mean'] - dat[dat$election == 'keyes_pct_ei','x.mean']
	
		par(las = 1)
		par(mar = c(5.1, 4.1, .5, 1.5))
		plot(seq(1:4),
			rep(1,4),
			ylim = ylims,
			xlim = xlims, 
			type = 'n',
			xaxt = 'n',
			yaxt = 'n',
			xlab = 'Election',
			ylab = ifelse(i == '5b','','Treatment Effect')
			)
		abline(h=0, lty = 2)
		
		if(i %in% c('5a','5b')){
			segments(
				x0= seq(1:4)-offsets, 
				x1 = seq(1:4)-offsets,
				y0 = dat[dat$group == 'white'&dat$election %in% pres.elections,'diff']-(1.96*dat[dat$group == 'white'&dat$election %in% pres.elections,'sd']),
				y1 =	dat[dat$group == 'white'&dat$election %in% pres.elections,'diff']+(1.96*dat[dat$group == 'white'&dat$election %in% pres.elections,'sd'])	
					)
			points(seq(1:4)-offsets,
				dat[dat$group == 'white'&dat$election %in% pres.elections,'diff'],
					pch = 21, 
					bg = 'white',
					col = 'black',
					cex = 2
				)
			segments(
				x0= seq(1:4)+offsets, 
				x1 = seq(1:4)+offsets,
				y0 = dat[dat$group == 'black'&dat$election %in% pres.elections,'diff']-(1.96*dat[dat$group == 'black'&dat$election %in% pres.elections,'sd']),
				y1 =	dat[dat$group == 'black'&dat$election %in% pres.elections,'diff']+(1.96*dat[dat$group == 'black'&dat$election %in% pres.elections,'sd'])	
					)
			points(seq(1:4)+offsets,
				dat[dat$group == 'black'&dat$election %in% pres.elections,'diff'],
					pch = 16,
					cex = 2
				)
			axis(side = 1, at = seq(1:4), 
				c('1996','2000','2004','2008'), 
				tick = F,
				cex.axis = cex.axis)		
			}
		else{
			segments(
				x0= seq(1:3)-offsets, 
				x1 = seq(1:3)-offsets,
				y0 = dat[dat$group == 'white'&dat$election %in% obama.elections,'diff']-(1.96*dat[dat$group == 'white'&dat$election %in% obama.elections,'sd']),
				y1 =	dat[dat$group == 'white'&dat$election %in% obama.elections,'diff']+(1.96*dat[dat$group == 'white'&dat$election %in% obama.elections,'sd'])	
					)
			points(seq(1:3)-offsets,
				dat[dat$group == 'white'&dat$election %in% obama.elections,'diff'],
					pch = 21, 
					bg = 'white',
					col = 'black',
					cex = 2
				)
			segments(
				x0= seq(1:3)+offsets, 
				x1 = seq(1:3)+offsets,
				y0 = dat[dat$group == 'black'&dat$election %in% obama.elections,'diff']-(1.96*dat[dat$group == 'black'&dat$election %in% obama.elections,'sd']),
				y1 =	dat[dat$group == 'black'&dat$election %in% obama.elections,'diff']+(1.96*dat[dat$group == 'black'&dat$election %in% obama.elections,'sd'])	
					)
  			points(seq(1:3)+offsets,
				dat[dat$group == 'black'&dat$election %in% obama.elections,'diff'],
					pch = 16,
					cex = 2
				)
		axis(side = 1, at = seq(1:3), 
					c('2004 \n Senate Primary','2004 \n Senate General','2008 \n President Primary'),
					tick = F,
					cex.axis = cex.axis
					)
		
			}	
		axis(side = 2,
			at = seq(from = -.1, to = .3, by = .05),
			label = c('-0.10','-0.05','0.00','0.05','0.10','0.15','0.20','0.25','0.30'),
			cex.axis = cex.axis
			)			
}		
```

```{r }
#parallel trends graph, Appendix Figures A2

# You guessed it

groups = read_csv('houses.data/par.trends.csv')


#plot elections votes

		par(las = 1)
		par(mar = c(5.1, 4.1, .5, .5))
		plot(1,
			.5,
			ylim = c(.5,.9),
			xlim = c(1,5),
			type = 'n',
			xaxt = 'n',
			ylab = 'Percent Voter Turnout',
			xlab = 'Year'
			)
 			text(seq(1:5),groups[,'white.control'], expression('W'['C']), cex = 1.5)
			text(seq(1:5),groups[,'white.treatment'],expression('W'['T']), cex = 1.5)
			text(seq(1:5),groups[,'black.control'], expression('B'['C']), cex = 1.5)
			text(seq(1:5),groups[,'black.treatment'],expression('B'['T']), cex = 1.5)
				
			lines(seq(1:5),groups[,'white.control'], lty = 2)
			lines(seq(1:5),groups[,'white.treatment'], lty = 2)
			lines(seq(1:5),groups[,'black.control'], lty = 2)
			lines(seq(1:5),groups[,'black.treatment'], lty = 2)

		axis(side = 1, at = seq(1:5), labels = c('1996','1998','2000','2002','2004'))
		axis(side = 2, at = seq(from = .5, to = .9, by = .05), labels = seq(from = .5, to = .9, by = .05))
			
```

## References

Enos, Ryan D., 2014, "Replication data for: What the Demolition of Public Housing Teaches Us About the Impact of Racial Threat on Political Behavior", https://doi.org/10.7910/DVN/26612, Harvard Dataverse, V2